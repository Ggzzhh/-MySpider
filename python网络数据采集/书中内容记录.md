## Beautiful4 - bs4
* `find(tag, attributes, recursive, text, keywords)`
* `findAll(tag, attributes, recursive, text, limit, keywords) `
    ```text
      tag - 标签名 如： span
      attributes - 属性 如: bsObj.findAll("span", {"color": "red"})
      recursive - 递归参数 布尔值 如果为False 则只抓取一级标签
      text - 文本参数 如: bsObj.findAll(text='the prince') 查找所有包含该文本的标签
      limit - 范围限制参数 如果只想抓取前20个参数则设置 limit=20
      keyword - 可以指定属性 如 id='text' （不推荐）
    ```

* `bsObj.find(……).children` 查找所有的子标签

* `bsObj.find(……).tr.next_siblings` 查找tr之后所有tr的兄弟标签

* `bsObj.find(……).tr.previous_siblings` 查找tr之前所有tr的兄弟标签
    * 当然，还有 next_sibling 和 previous_sibling 函数，与 next_siblings 和 
    previous_siblings 的作用类似，只是它们返回的是单个标签，而不是一组标签。

* 处理父标签 parent 和 parents

* 获取属性
    获取标签的全部属性 `tag.attrs`
    获取部分属性 `tag.attrs["src"]`
    
* 可以在bs中使用 正则表达式(re) 未命名函数(Lambda) 作为条件
    `bsObj.findAll(lambda tag: len(tag.attrs) == 2)`

## 正则表达式 Regex
?! 不包含 如： ^((?![A-Z]).)*$   开头不能为大写字母


## 简单爬虫例子
    1 获取页面上的所有外链
    2 还有外链吗？
    是 --- 返回一个随机外链
    否 --- 进入页面上的一个内链 继续从 1 开始
    在以任何正式的目的运行代码之前，请确认你已经在可能出现问题的地方都 放置了检查语句。


## `urllib.parsed` 属性
```python
from urllib.request import urlparse
parsed = urlparse('url地址')
parsed.scheme   #网络协议
parsed.netloc   #服务器位置 
parsed.path     #网页文件在服务器中存放的位置
parsed.params   #可选参数
parsed.query    #连接符（&）连接键值对
parsed.fragment #拆分文档中的特殊猫
parsed.username #用户名
parsed.password #密码
parsed.hostname #服务器名称或者地址
parsed.port     #端口（默认是80)
```

## 需要保存时最好保存需要下载的文件的url 不要直接下载该文件

## pymysql 操作时需要注意:
    # sql语句中有中文的时候进行encode
    # 连接的时候加入charset设置信息 charset="utf8"

## 所以最好一开始就让你的数据库支持Unicode：
```sql
ALTER DATABASE scraping CHARACTER SET = utf8mb4 COLLATE = utf8mb4_unicode_ci;
ALTER TABLE pages CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
ALTER TABLE pages CHANGE title title VARCHAR(200) CHARACTER SET utf8mb4 COLLATE
utf8mb4_unicode_ci;
ALTER TABLE pages CHANGE content content VARCHAR(10000) CHARACTER SET utf8mb4 CO
LLATE utf8mb4_unicode_ci;
```

## 防止数据太大导致内存堆栈溢出 为函数加一个参数 函数开始时判断这个参数是否大于4？
    这样可以在函数调用自身时（该参数+1）防止递归次数过多
    如：getLinks(newPage, recursionLevel+1)
    
## 转为utf-8编码
```python
str("要转换的内容", "utf-8")
```    

## 操作pdf文件可用：pdfminer3k


## 数据大部分时间需要进行数据清洗


## operator.itemgetter(1) -- 这个函数的作用是提取列表或者元组中下标为1的数据 


## 有空闲的话最好学一下自然语言那本书


## 爬虫利器selenium+phantomJS 采集动态页面(chapter10)


## 利用PIL预处理图片,然后使用tesseract——orc 辨认验证码  高级点的使用机器学习(待学)


## 让机器人看起来像人类用户
    1. 修改请求头headers： chapter12-1
    2. 处理cookie： chapter12-2
    3. 如果可以 采集每个页面之后让程序休息3秒 time.sleep(3)
    4. 避免蜜罐  因为selenium可以获取访问页面的内容，通过id_displayed()判断是否可见 chapter12-3
        