## 检查robots.txt

## 使用python自带的rebotparser解析robots.txt
```python
from urllib.robotparser import RobotFileParser
rp = RobotFileParser()
```

## 使用bs4解析时能用lxml最好用lxml解析速度最快
    下载地址: https://pypi.python.org/pypi/lxml
    安装：在文件所在目录下使用pip install xxxx
    bs = BeautifulSoup(html, 'lxml')


### bs4文档地址：http://beautifulsoup.readthedocs.io/zh_CN/latest/


## 为了进行跨平台的存储 我们需要将URL安全的映射为跨平台的文件名
    为了保证在不同系统中，我们的路径都是安全的，需要限制只能包含数字字母和基本符号，
    将其他的替换为下划线
    re.sub('[^/0-9a-zA-Z\-.,;_]', '_', url)